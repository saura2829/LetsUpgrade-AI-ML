{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 1:\n",
    "Create one array of actual values and another array of predicted values. Compare the two sets with the confusion matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion_matrix:\n",
      " [[3 2]\n",
      " [1 4]]\n",
      "\n",
      "Accuracy = 0.7\n",
      "\n",
      "Recall(+ve) : 0.8\n",
      "Recall(-ve) : 0.6\n",
      "\n",
      "Precision(+ve) : 0.6666666666666666\n",
      "Precision(-ve) : 0.75\n",
      "\n",
      "f1 score(+ve) : 0.7272727272727272\n",
      "f1 score(-ve) : 0.6666666666666665\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.75      0.60      0.67         5\n",
      "           1       0.67      0.80      0.73         5\n",
      "\n",
      "    accuracy                           0.70        10\n",
      "   macro avg       0.71      0.70      0.70        10\n",
      "weighted avg       0.71      0.70      0.70        10\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Importing required modules\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report, confusion_matrix\n",
    "\n",
    "# Actual Values\n",
    "actual = [1, 1, 0, 1, 0, 0, 1, 0, 1, 0]\n",
    "\n",
    "# Predicted Values\n",
    "pred = [1, 1, 1, 0, 1, 0, 1, 0, 1, 0]\n",
    "\n",
    "# Confusion Matrix\n",
    "print(\"Confusion_matrix:\\n\",\n",
    "      confusion_matrix(y_true=actual, y_pred=pred, labels=[0, 1]),\n",
    "      end=\"\\n\\n\")\n",
    "\n",
    "# Accuracy\n",
    "print(\"Accuracy =\", accuracy_score(y_true=actual, y_pred=pred), end=\"\\n\\n\")\n",
    "\n",
    "# Recall\n",
    "positive = recall_score(y_true=actual, y_pred=pred, pos_label=1)\n",
    "negetive = recall_score(y_true=actual, y_pred=pred, pos_label=0)\n",
    "print(\"Recall(+ve) :\", positive, end=\"\\n\")\n",
    "print(\"Recall(-ve) :\", negetive, end=\"\\n\\n\")\n",
    "\n",
    "# Precision\n",
    "positive1 = precision_score(y_true=actual, y_pred=pred, pos_label=1)\n",
    "negetive1 = precision_score(y_true=actual, y_pred=pred, pos_label=0)\n",
    "print(\"Precision(+ve) :\", positive1, end=\"\\n\")\n",
    "print(\"Precision(-ve) :\", negetive1, end=\"\\n\\n\")\n",
    "\n",
    "# f1 score\n",
    "positive2 = f1_score(y_true=actual, y_pred=pred, pos_label=1)\n",
    "negetive2 = f1_score(y_true=actual, y_pred=pred, pos_label=0)\n",
    "print(\"f1 score(+ve) :\", positive2, end=\"\\n\")\n",
    "print(\"f1 score(-ve) :\", negetive2, end=\"\\n\\n\")\n",
    "\n",
    "print(classification_report(y_true=actual, y_pred=pred, labels=[0, 1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 2:\n",
    "Find out the recall, precision, F1 score and confusion matrix with picture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix:\n",
      " [[48  8]\n",
      " [18 32]]\n",
      "\n",
      "Accuracy = 0.7547169811320755\n",
      "\n",
      "Recall = 0.7272727272727273\n",
      "\n",
      "Precision = 0.8571428571428571\n",
      "\n",
      "f1 score = 0.7868852459016394\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Confusion Matrix\n",
    "cm = np.array([[48, 8], [18, 32]])\n",
    "print(\"Confusion Matrix:\\n\", cm, end=\"\\n\\n\")\n",
    "\n",
    "# True positive\n",
    "tp = cm[0, 0]\n",
    "# True negetive\n",
    "tn = cm[1, 1]\n",
    "# False positive\n",
    "fp = cm[0, 1]\n",
    "# False negetive\n",
    "fn = cm[1, 0]\n",
    "\n",
    "# Accuracy Score\n",
    "acc = (tp + tn) / (tp + tn + fp + fn)\n",
    "print(\"Accuracy =\", acc, end=\"\\n\\n\")\n",
    "\n",
    "# Recall\n",
    "rc = tp / (tp + fn)\n",
    "print(\"Recall =\", rc, end=\"\\n\\n\")\n",
    "\n",
    "# Precision\n",
    "pr = tp / (tp + fp)\n",
    "print(\"Precision =\", pr, end=\"\\n\\n\")\n",
    "\n",
    "# f1 score\n",
    "fs = (2 * pr * rc) / (pr + rc)\n",
    "print(\"f1 score =\", fs, end=\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
